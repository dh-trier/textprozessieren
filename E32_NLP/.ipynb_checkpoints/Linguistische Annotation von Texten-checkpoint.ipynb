{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0ac680",
   "metadata": {},
   "source": [
    "# Linguistische Annotation von Texten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fbb7a",
   "metadata": {},
   "source": [
    "Die linguistische Annotation von Texten meint insbesondere die folgenden Schritte, die oft ein Teil des Präprozessierens von Texten für die weiterführenden Analysen sind: \n",
    "\n",
    "1. Tokenisierung\n",
    "2. Lemmatisierung\n",
    "3. POS-Tagging\n",
    "4. Named Entity Recognition\n",
    "\n",
    "Es gibt für die linguistische Annotation eine ganze Reihe von einschlägigen Tools, die Sie kennen sollten: \n",
    "\n",
    "1. NLTK (Natural Language Tool-Kit): \n",
    "1. Stanford NLP => Stanza\n",
    "1. spaCy \n",
    "\n",
    "Heute fokussieren wir auf spaCy, weil das ein relativ neues, modernes, sehr aktiv entwickeltes und für diverse Sprachen geeignetes Annotationstool ist. Beispielsweise sind auch Methoden verfügbar, die auf neuronalen Netzen basieren (Deep Learning) oder können ggfs. vorhandene GPUs beim Prozessieren genutzt werden. \n",
    "\n",
    "* Homepage: https://spacy.io/\n",
    "* Dokumentation zur Annotation: https://spacy.io/usage/linguistic-features\n",
    "\n",
    "Als Bonus können wir mit spaCy auch recht einfach Wortvektoren abrufen, die die Semantik der Wörter im Text repräsentieren. Damit kann man beispielsweise sehr einfach Wörter mit ähnlicher Bedeutung (Synonyme) in einem Text identifizieren. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d6b2f",
   "metadata": {},
   "source": [
    "## Korpus\n",
    "\n",
    "Wir verwenden als Beispielkorpus die drei kurzen deutschen Erzähltexte, die unter dem Titel \"narration\" im `datasets`-Repository verfügbar sind: https://github.com/dh-trier/datasets/ (im Ordner \"corpora\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762015f",
   "metadata": {},
   "source": [
    "## Installation von spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daab0fd",
   "metadata": {},
   "source": [
    "Informationen zur Installation: https://spacy.io/usage\n",
    "\n",
    "Deutsche Annotationsmodelle:\n",
    "\n",
    "* auf der Kommandozeile (effizientes Modell): `python3 -m spacy download de_core_news_sm` \n",
    "* Auf accuracy optimiertes Modell: `python3 -m spacy download de_dep_news_trf`. \n",
    "* Ein besonders umfangreiches Modell mit sehr vielen Wortvektoren: `python3 -m spacy download de_core_news_lg`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308039c8",
   "metadata": {},
   "source": [
    "## Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basisimporte\n",
    "from os.path import join\n",
    "from os.path import basename\n",
    "from glob import glob\n",
    "\n",
    "# Spezielle Importe\n",
    "import spacy\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408aa86",
   "metadata": {},
   "source": [
    "## Texte laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f70f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusfolder = join(\"..\", \"..\", \"datasets\", \"corpora\", \"narration\", \"*.txt\")\n",
    "\n",
    "def load_text(textfile): \n",
    "    with open(textfile, \"r\", encoding=\"utf8\") as infile: \n",
    "        text = infile.read()\n",
    "    return text\n",
    "\n",
    "def main(corpusfolder): \n",
    "    corpus = {}\n",
    "    for textfile in glob(corpusfolder): \n",
    "        idno = basename(textfile).split(\".\")[0]\n",
    "        corpus[idno] = load_text(textfile)\n",
    "    for idno,text in corpus.items(): \n",
    "        print(idno, text[0:50])\n",
    "    return corpus\n",
    "\n",
    "corpus = main(corpusfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33989ec9",
   "metadata": {},
   "source": [
    "## spaCy verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotationspipeline mit spezifischem Modell initialisieren\n",
    "nlp = spacy.load(\"de_core_news_sm\")  # Oder: \"de_dep_news_trf\"\n",
    "\n",
    "def annotate(nlp, text): \n",
    "    annotated = nlp(text)\n",
    "    return annotated\n",
    "\n",
    "def main(corpus, nlp): \n",
    "    acorpus = {}\n",
    "    for idno,text in corpus.items(): \n",
    "        acorpus[idno] = nlp(text)\n",
    "    return acorpus\n",
    "\n",
    "acorpus = main(corpus, nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e45388",
   "metadata": {},
   "source": [
    "## Auf Annotationen zugreifen\n",
    "\n",
    "Ein annotierter Text ist in spaCy eine spezifische Struktur, nämlich eine Liste von Tokens, wobei jedes Token ein Objekt mit einer Reihe von Methoden ist. Diese Methoden erlauben den Zugriff auf die verschiedenen Annotationsebenen. Dazu gehören die folgenden Methoden bzw. Annotationen: \n",
    "\n",
    "* `token.text`: die Wortform\n",
    "* `token.lemma_`: das Lemma\n",
    "* `token.pos_`: das POS-Tag (einfaches UPOS Tag)\n",
    "* `token.tag_`: das POS-Tag (vollständiges / detailliertes Tag)\n",
    "* `token.dep_`: die syntaktische Annotation (dependency)\n",
    "* `token.shape_`: Gibt es Kapitalisierung, Interpunktion, Zahlen?\n",
    "* `token.is_alpha`: besteht das Token aus Buchstaben? (True/False)\n",
    "* `token.is_stop`: ist das Token ein Stopword? (True/False)\n",
    "* `token.morph`: detaillierte morphologische Information (u.a. Kasus, Genus, Numerus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa376c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_annotations(atext): \n",
    "        for token in atext[10:15]: \n",
    "            print(token.text, \"(lemma, pos, tag)\", token.lemma_, token.pos_, token.tag_)\n",
    "            print(token.text, \"(shape, alpha, stop)\", token.shape_, token.is_alpha, token.is_stop)\n",
    "            print(token.text, \"(morph)\", token.morph)\n",
    "            print(token.text, \"(dep)\", token.dep_, \"\\n\")\n",
    "\n",
    "\n",
    "def get_annotation(atext):\n",
    "    alist = [token.pos_ for token in atext]\n",
    "    print(alist[0:40])\n",
    "    return alist\n",
    "                      \n",
    "            \n",
    "def retrieve_annotations(acorpus): \n",
    "    for idno,atext in acorpus.items(): \n",
    "        print(\"===\\nText:\", idno)\n",
    "        #show_annotations(atext)         # Für die visuelle Inspektion\n",
    "        alist = get_annotation(atext)   # Um eine Annotationsschicht weiter zu verarbeiten\n",
    "\n",
    "retrieve_annotations(acorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151fdb77",
   "metadata": {},
   "source": [
    "## Named Entities\n",
    "\n",
    "Auch die Named Entities sind in der Standard-Annotation direkt enthalten, allerdings in einem Unter-Objekt, `.ent`. Hier sind nur diejenigen Token enthalten, die auch tatsächlich als Entität erkannt wurden. \n",
    "\n",
    "Die folgenden Eigenschaften können abgerufen werden: \n",
    "\n",
    "* `ent.text`: Wortform\n",
    "* `ent.label_`: NE-Kategorie\n",
    "* `ent.start_char`: Position des Anfangszeichens im Text\n",
    "* `ent.end_char`: Position des letzten Zeichens im Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33105d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_entities(atext): \n",
    "        for ent in atext.ents[10:15]: \n",
    "            print(ent.text, ent.label_, ent.start_char, ent.end_char)\n",
    "                   \n",
    "            \n",
    "def retrieve_entities(acorpus): \n",
    "    for idno,atext in acorpus.items(): \n",
    "        print(\"===\\nText:\", idno)\n",
    "        show_entities(atext) \n",
    "\n",
    "retrieve_entities(acorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035213b0",
   "metadata": {},
   "source": [
    "Tja, man sieht dass das nicht besonders gut funktioniert. Es dürfte daran liegen, dass das Modell mit modernen Nachrichtentexten trainiert wurde, während wir hier eben fiktionale Erzähltexte aus dem frühen 20. Jahrhundert vorliegen haben. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e7021",
   "metadata": {},
   "source": [
    "## Kleiner Test auf einem Wikipedia-Artikel\n",
    "\n",
    "1. Wir laden mit dem Modul `wikipedia` einen Artikel aus Wikipedia herunter, direkt als plain text. Siehe die Dokumentation hier: https://wikipedia.readthedocs.io/en/latest/quickstart.html#quickstart\n",
    "2. Wir annotieren den Text und lassen uns dann die Named Entities herausgeben. \n",
    "\n",
    "(Eine Alternative für den Download des Textes wäre natürlich auch `requests`, mit dem man die API von Wikipedia auch gut nutzen kann. Siehe die Hinweise hier: https://stackoverflow.com/questions/4452102/how-to-get-plain-text-out-of-wikipedia). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wptitle = \"Riesling\"    # \"Riesling\", Paul Otlet\", \"Alan Turing\" (?), \"Susan Sontag\", \"Mosel\" (?)\n",
    "lang = \"de\"\n",
    "\n",
    "def get_wptext(wptitle, lang): \n",
    "    \"\"\"\n",
    "    Lade einen bestimmten Wikipedia-Artikel in einer bestimmten Sprache herunter. \n",
    "    Gibt den plain text des vollständigen Artikels zurück. \n",
    "    \"\"\"\n",
    "    wikipedia.set_lang(lang)\n",
    "    resp = wikipedia.page(wptitle)\n",
    "    print(resp.title, resp.url)\n",
    "    return resp.content\n",
    "\n",
    "def annotate_wptext(wptext): \n",
    "    \"\"\"\n",
    "    Annotiere den Wikipedia-Artikel. \n",
    "    Zeige die Named Entities. \n",
    "    \"\"\"\n",
    "    atext = nlp(wptext)\n",
    "    for ent in atext.ents: \n",
    "        print(ent.text, ent.label_,)\n",
    "\n",
    "        \n",
    "def main(wptitle, lang): \n",
    "    wptext = get_wptext(wptitle, lang)\n",
    "    annotate_wptext(wptext)\n",
    "    #print(wptext)\n",
    "\n",
    "main(wptitle, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47e9e8",
   "metadata": {},
   "source": [
    "In der Tat sieht man hier sehr schön, dass das mit einem solchen Sachtext sehr gut funktioniert. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a3451",
   "metadata": {},
   "source": [
    "## Wortvektoren: Semantische Ähnlichkeit\n",
    "\n",
    "Wir laden noch einmal einen Wikipedia-Artikel aus dem obigen Beispiel und suchen jetzt in diesem Text Paare von Wörtern, die sich besonders ähnlich sind.\n",
    "\n",
    "Damit das gut funktioniert, brauchen wir ein größeres Sprachmodell, das auch (viele) Wortvektoren enthält. Daher bitte noch das \"de_core_news_lg\" herunterladen, wie oben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170252e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "wikipedia.set_lang(\"de\")\n",
    "text = wikipedia.page(\"Riesling\").content\n",
    "atext = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc02fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_atext(atext): \n",
    "    wordlist = []\n",
    "    vectors = []\n",
    "    for token in atext: \n",
    "        if token.is_oov == False: \n",
    "            if token.pos_ == \"NOUN\": # VERB|NOUN\n",
    "                if token.text not in wordlist:     # Wir wollen Duplikate der Wortformen vermeiden\n",
    "                    wordlist.append(token.text)\n",
    "                    vectors.append(token)\n",
    "    return vectors\n",
    "\n",
    "def get_similarities(vectors):\n",
    "    pairs = set(combinations(vectors, 2))\n",
    "    results = {}\n",
    "    for item in pairs: \n",
    "        results[item[0].text + \" ~ \" + item[1].text] = item[0].similarity(item[1])\n",
    "    results = pd.Series(results)\n",
    "    results.sort_values(ascending=False, inplace=True)\n",
    "    return results\n",
    "\n",
    "def find_similar_words(atext): \n",
    "    vectors = filter_atext(atext)\n",
    "    results = get_similarities(vectors)\n",
    "    \n",
    "    print(\"Most similar word pairs:\")\n",
    "    print(results.head(30))\n",
    "    print(\"\\nLeast similar word pairs:\")\n",
    "    print(results.tail(10))\n",
    "\n",
    "find_similar_words(atext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9592d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python397jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
