{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b500bce",
   "metadata": {},
   "source": [
    "# Webscraping mit requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65768e",
   "metadata": {},
   "source": [
    "Gegenstand dieses Inputreferetas ist das Webscraping mit der Python-Library `requests`. Die Dokumentation zur Library finden Sie hier: https://docs.python-requests.org/en/master/. \n",
    "\n",
    "Wir machen das am Beispiel von Texten der Seite Archive of Our Own (AO3), einer Fanfiction-Seite: https://archiveofourown.org/. \n",
    "\n",
    "Die Grundidee ist, eine Suche nach bestimmten Texten nachzubauen; dann die Identifier der relevanten Texte einzusammeln; und dann die Texte in HTML herunterzuladen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c071c92",
   "metadata": {},
   "source": [
    "## Die Arbeitsschritte im Einzelnen\n",
    "\n",
    "1. Suchabfrage überlegen und im Webinterface vornehmen\n",
    "2. Suchabfrage mit requests \"nachbauen\"\n",
    "3. Liste der Text-IDs einsammeln\n",
    "4. Alle Texte mit den entsprechenden IDs herunterladen\n",
    "\n",
    "Mit dem Input aus dem Modul \"Auszeichnungssprachen\" zur Verarbeitung von HTML mit BeautifulSoup oder lxml (Titel: Markup und Python) können Sie die folgenden Schritte dann auch selbst durchführen:  \n",
    "\n",
    "5. Metadaten aus den HTML-Dateien extrahieren und abspeichern (\"A3O_metadata.tsv\")\n",
    "6.Volltext aus den HTML-Dateien extrahieren und abspeichern. \n",
    "\n",
    "Dann ist eine Analyse des Korpus möglich."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0be0b",
   "metadata": {},
   "source": [
    "## Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "275d9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "# Specific imports\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2e912",
   "metadata": {},
   "source": [
    "## Schritt 1: Die Suchabfrage formulieren\n",
    "\n",
    "Einstieg: https://archiveofourown.org/works/search. Wir suchen in diesem Beispiel Texte, die den folgenden Kriterien entsprechen: \n",
    "\n",
    "- Deutsche Texte\n",
    "- 10000-20000 Wörter Länge\n",
    "- Harry Potter-Fandom\n",
    "- No Crossovers\n",
    "- No Archive Warnings\n",
    "- Komplette Texte\n",
    "\n",
    "Das ergibt im Januar 2022 45 Treffer. Der Link mit den Suchergebnissen ist folgender: https://archiveofourown.org/works/search?utf8=%E2%9C%93&work_search%5Bquery%5D=&work_search%5Btitle%5D=&work_search%5Bcreators%5D=&work_search%5Brevised_at%5D=&work_search%5Bcomplete%5D=T&work_search%5Bcrossover%5D=F&work_search%5Bsingle_chapter%5D=0&work_search%5Bword_count%5D=10000-20000&work_search%5Blanguage_id%5D=de&work_search%5Bfandom_names%5D=Harry+Potter+-+J.+K.+Rowling&work_search%5Brating_ids%5D=&work_search%5Barchive_warning_ids%5D%5B%5D=16&work_search%5Bcharacter_names%5D=&work_search%5Brelationship_names%5D=&work_search%5Bfreeform_names%5D=&work_search%5Bhits%5D=&work_search%5Bkudos_count%5D=&work_search%5Bcomments_count%5D=&work_search%5Bbookmarks_count%5D=&work_search%5Bsort_column%5D=_score&work_search%5Bsort_direction%5D=desc&commit=Search\n",
    "\n",
    "In diesem Link sind alle Suchkriterien kodiert, sodass wir den Link gut nachbauen und die Suchabfrage auch aus `requests` heraus formulieren können. \n",
    "\n",
    "Alle Suchkriterien, die hier keinen Eintrag haben (bei denen auf `=` direkt das `&` folgt, können ignoriert werden. Dadurch vereinfacht sich die Sache deutlich. \n",
    "\n",
    "Außerdem erlaubt die Library `requests`, mit der wir das Ganze machen, solche Suchbedingungen als Parameter zu übergeben. Die Library konstruiert dann selbständig eine korrekte Suchanfrage aus dem Basislink der Suche, der Anzahl an Trefferseiten, die berücksichtigt werden sollen, und den Suchkriterien. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7749efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://archiveofourown.org/works/search?commit=Search&utf8=%25E2%259C%2593&work_search%5Bcomplete%5D=T&work_search%5Bcrossover%5D=F&work_search%5Bfandom_names%5D=Harry%2BPotter%2B-%2BJ.%2BK.%2BRowling&work_search%5Blanguage_id%5D=de&work_search%5Bsingle_chapter%5D=0&work_search%5Bsort_column%5D=_score&work_search%5Bsort_direction%5D=desc&work_search%5Bword_count%5D=10000-20000&work_search%5Barchive_warning_ids%5D%5B%5D=16&page=1\n",
      "20 ['27328087', '435741', '31371968', '29333343', '20115061', '655562', '33957355', '11567019', '36319510', '30379272', '1011701', '8377495', '27306145', '11558709', '13808433', '34077589', '11566914', '8142823', '12882969', '8263144']\n",
      "https://archiveofourown.org/works/search?commit=Search&utf8=%25E2%259C%2593&work_search%5Bcomplete%5D=T&work_search%5Bcrossover%5D=F&work_search%5Bfandom_names%5D=Harry%2BPotter%2B-%2BJ.%2BK.%2BRowling&work_search%5Blanguage_id%5D=de&work_search%5Bsingle_chapter%5D=0&work_search%5Bsort_column%5D=_score&work_search%5Bsort_direction%5D=desc&work_search%5Bword_count%5D=10000-20000&work_search%5Barchive_warning_ids%5D%5B%5D=16&page=2\n",
      "20 ['26920879', '8641453', '34891537', '25972450', '1031352', '10980477', '8463418', '8679988', '31372718', '15402684', '21853621', '27698624', '23929474', '30881075', '183049', '4317018', '30342693', '23664853', '27913333', '10960482']\n",
      "https://archiveofourown.org/works/search?commit=Search&utf8=%25E2%259C%2593&work_search%5Bcomplete%5D=T&work_search%5Bcrossover%5D=F&work_search%5Bfandom_names%5D=Harry%2BPotter%2B-%2BJ.%2BK.%2BRowling&work_search%5Blanguage_id%5D=de&work_search%5Bsingle_chapter%5D=0&work_search%5Bsort_column%5D=_score&work_search%5Bsort_direction%5D=desc&work_search%5Bword_count%5D=10000-20000&work_search%5Barchive_warning_ids%5D%5B%5D=16&page=3\n",
      "5 ['9367178', '2765708', '1015793', '8155888', '8554963']\n",
      "Total number of ids collected: 45\n"
     ]
    }
   ],
   "source": [
    "# Parameter\n",
    "\n",
    "queryurl = \"https://archiveofourown.org/works/search?commit=Search\"\n",
    "numpages = 3\n",
    "querycriteria = {\n",
    "    \"utf8\" : \"%E2%9C%93\",\n",
    "    \"work_search[complete]\": \"T\",\n",
    "    \"work_search[crossover]\" : \"F\",\n",
    "    \"work_search[fandom_names]\" : \"Harry+Potter+-+J.+K.+Rowling\",\n",
    "    \"work_search[language_id]\" : \"de\",\n",
    "    \"work_search[single_chapter]\" : \"0\",\n",
    "    \"work_search[sort_column]\" : \"_score\",\n",
    "    \"work_search[sort_direction]\" : \"desc\",\n",
    "    \"work_search[word_count]\" : \"10000-20000\",\n",
    "    \"work_search[archive_warning_ids][]\" : \"16\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Funktionen\n",
    "\n",
    "def get_html(queryurl, querycriteria): \n",
    "    try:\n",
    "        queryresponse = requests.get(queryurl, params=querycriteria, timeout=4)\n",
    "        #print(queryresponse.url)\n",
    "        queryhtml = queryresponse.text\n",
    "        #print(queryhtml)\n",
    "        return queryhtml\n",
    "    except:\n",
    "        print(\"Error receiving HTML\")\n",
    "    \n",
    "\n",
    "def get_ids(queryhtml): \n",
    "    try: \n",
    "        ids = re.findall(r\"<a href=\\\"/works/(\\d*?)\\\">\", queryhtml)\n",
    "        print(len(ids), ids)\n",
    "        return ids\n",
    "    except: \n",
    "        print(\"Error extracting IDs\")\n",
    "\n",
    "            \n",
    "# Aufruf\n",
    "\n",
    "all_ids = []\n",
    "for page in range(1,numpages+1):\n",
    "    querycriteria[\"page\"] = str(page)\n",
    "    queryhtml = get_html(queryurl, querycriteria)\n",
    "    all_ids.extend(get_ids(queryhtml))\n",
    "print(\"Total number of ids collected:\", len(all_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d629c",
   "metadata": {},
   "source": [
    "## Herunterladen der HTML-Dateien nach ID\n",
    "\n",
    "Jetzt soll für jeden Identifier der entsprechende Text heruntergeladen werden. Hier können wir nun statt mit den Parametern zu arbeiten, den einfacheren Weg gehen, die passende URL sozusagen von Hand zusammenzusetzen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "004d0c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved A3O-27328087.html\n",
      "Saved A3O-435741.html\n",
      "Saved A3O-31371968.html\n",
      "Saved A3O-29333343.html\n",
      "Saved A3O-20115061.html\n"
     ]
    }
   ],
   "source": [
    "# Parameter\n",
    "\n",
    "A3O_base = \"https://archiveofourown.org/\"\n",
    "htmlfolder=join(\"..\", \"data\", \"A3O\", \"html\", \"\")\n",
    "\n",
    "\n",
    "# Funktionen\n",
    "    \n",
    "def get_html(item): \n",
    "    \"\"\"\n",
    "    Lade für das jeweilige item (eine ID) die HTML-Datei herunter. \n",
    "    Gibt einen String zurück. \n",
    "    \"\"\"\n",
    "    url = A3O_base + \"works/\" + item + \"?view_full_work=true\"\n",
    "    #print(url)\n",
    "    response = requests.get(url, timeout=4)\n",
    "    html = response.text\n",
    "    #print(html[15000:16000])\n",
    "    return html\n",
    "\n",
    "\n",
    "def save_html(item, html):\n",
    "    \"\"\"\n",
    "    Speichere den String in eine HTML-Datei,\n",
    "    die den Identifier im Dateinamen hat. \n",
    "    \"\"\"\n",
    "    filename = join(htmlfolder, \"A3O-\" + str(item) + \".html\")\n",
    "    with open(filename, \"w\") as outfile: \n",
    "        outfile.write(html)\n",
    "    print(\"Saved\", os.path.basename(filename))\n",
    "\n",
    "\n",
    "def main(A3O_base, htmlfolder, all_ids): \n",
    "    for item in all_ids[0:5]:    # Zum Ausprobieren nur die ersten 5 Texte. \n",
    "        html = get_html(item)\n",
    "        save_html(item, html)\n",
    "\n",
    "main(A3O_base, htmlfolder, all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53464c",
   "metadata": {},
   "source": [
    "Das ist auch schon alles! Das neue kleine Korpus ist jetzt auf der Festplatte gespeichert, jeder Text in einer HTML-Datei. Jetzt könnten Sie aus den HTML-Dateien die Metadaten und den Volltext extrahieren, um ein analysierbares Korpus zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be8294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python397jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
