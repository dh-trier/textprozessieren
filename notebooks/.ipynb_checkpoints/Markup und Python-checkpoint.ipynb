{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0327d2",
   "metadata": {},
   "source": [
    "# Markup und/in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2187ed",
   "metadata": {},
   "source": [
    "## Einleitung \n",
    "\n",
    "Dieses Inputreferat ist Teil des Moduls Auszeichnungssprachen im Master of Science \"Digital Humanities\" der Universität Trier. Es ist aber fast ebenso relevant für das Modul \"Programmieren 1: Textprozessieren\". \n",
    "\n",
    "Thema ist die Nutzung und Verarbeitung von Markup-Dateien (wie HTML oder XML) mit Python. Hierfür werden mehrere relevante Libraries vorgestellt und einfache Nutzungsbeispiele gezeigt. \n",
    "\n",
    "Insbesondere geht es um die folgenden Aspekte: \n",
    "\n",
    "1. In HTML-Dateien Informationen suchen mit Regulären Ausdrücken\n",
    "1. HTML-Dateien durchsuchen oder bearbeiten mit BeautifulSoup\n",
    "1. In XML-Dateien mit Python Informationen suchen: XPath in lxml\n",
    "1. Validieren von XML-TEI mit Python: lxml\n",
    "1. XSL-Transformation auf XML anwenden mit lxml\n",
    "1. Dokumentation zu einem Schema generieren: RNG nach Markdown\n",
    "\n",
    "Dieses Inputreferat fokussiert eher auf die Szenarien und Ergebnisse, als auf die Details der konkreten Umsetzung. Vollständige und ausführbare Code-Beispiele werden aber selbstverständlich mit angegeben. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6da6f",
   "metadata": {},
   "source": [
    "## HTML durchsuchen mit Regulären Ausdrücken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f659a",
   "metadata": {},
   "source": [
    "Für diesen Anwendungsfall verwenden wir mehrere aus dem \"Archive of our Own\" heruntergeladene Textdateien in HTML. Das folgenden Szenario wird berücksichtigt:  Extraktion einiger Metadaten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from os.path import basename\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "\n",
    "htmlfiles = join(\"..\", \"data\", \"aao\", \"*.html\")\n",
    "\n",
    "def read_html(htmlfile): \n",
    "    \"\"\"\n",
    "    Open and read a single HTML file.\n",
    "    Returns the content as a string.\n",
    "    \"\"\"\n",
    "    with open(htmlfile, \"r\", encoding=\"utf8\") as infile: \n",
    "        html = infile.read()\n",
    "    #print(html[2000:3000])\n",
    "    return html\n",
    "\n",
    "def find_metadata(html): \n",
    "    \"\"\"\n",
    "    Search specific metadata items in the HTML string.\n",
    "    Returns one dictionary with metadata for one document. \n",
    "    \"\"\"\n",
    "    title = re.findall(\"<title>(.*?)</title>\", html)[0]\n",
    "    fandom = re.findall(\"<dt>Fandom:</dt>\\n.*?<dd><a href=.*?>(.*?)</a>\", html)[0]\n",
    "    metadata = {\"title\" : title, \"fandom\": fandom}\n",
    "    return metadata\n",
    "\n",
    "def save_metadata(metadata): \n",
    "    \"\"\"\n",
    "    Transform the dictionary to a DataFrame and save as TSV.\n",
    "    Returns a TSV file saved to disk. \n",
    "    \"\"\"\n",
    "    metadata = pd.DataFrame.from_dict(metadata).T\n",
    "    print(metadata)\n",
    "    with open(\"aao-metadata.tsv\", \"w\", encoding=\"utf8\") as outfile: \n",
    "        metadata.to_csv(outfile, sep=\"\\t\")\n",
    "\n",
    "        \n",
    "def main(htmlfile): \n",
    "    \"\"\"\n",
    "    Collect metadata for a collection of HTML files.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    for htmlfile in glob.glob(htmlfiles):\n",
    "        idno = basename(htmlfile).split(\".\")[0]\n",
    "        html = read_html(htmlfile)\n",
    "        metadata[idno] = find_metadata(html)\n",
    "    #print(metadata)\n",
    "    save_metadata(metadata)\n",
    "\n",
    "main(htmlfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbe01b",
   "metadata": {},
   "source": [
    "## Suchen in und Bearbeiten von HTML: mit BeautifulSoup\n",
    "\n",
    "Mit der Library `BeatifulSoup` kann man nicht nur nach Mustern suchen, sondern die Dokumentstruktur des HTML-Dokuments als solche nutzen und wesentlich systematischer Markup-Dateien durchsuchen und bearbeiten. BeautifulSoup funktioniert dabei nicht nur mit HTML, sondern auch mit XML. \n",
    "\n",
    "Dokumentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53875a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "def process_html(html): \n",
    "    hsoup = bs(html, 'html.parser')\n",
    "\n",
    "    # (1) Im HTML-Baum suchen: nach einem Element, Element-Inhalt, etc.\n",
    "    print(hsoup.h1)\n",
    "    #print(hsoup.title)\n",
    "    #print(hsoup.title.string)\n",
    "    #print(len(hsoup.find_all(\"p\")))\n",
    "\n",
    "    # (2) Allen Text aus dem \"body\" extrahieren (ohne Metadaten)\n",
    "    #print(hsoup.body.get_text())\n",
    "    #print(len(hsoup.body.get_text()))\n",
    "    #text = \"\"\n",
    "    #for item in hsoup.find_all(id=\"chapters\"): \n",
    "    #    text += item.get_text()\n",
    "    #print(text)\n",
    "    \n",
    "    # (3) Das HTML-Dokument verändern\n",
    "    #element = hsoup.title\n",
    "    #element.clear()      # removes the contents of an element\n",
    "    #element.extract()    # removes the whole element\n",
    "    #print(hsoup)\n",
    "    #hsoup.title.wrap(hsoup.new_tag(\"titleStmt\")) # Ein Element mit zusätzlichem Element umgeben\n",
    "    #print(hsoup)\n",
    "    \n",
    "def main(htmlfiles): \n",
    "    for htmlfile in glob.glob(htmlfiles): \n",
    "        html = read_html(htmlfile)\n",
    "        text = process_html(html)\n",
    "\n",
    "main(htmlfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7ce9f",
   "metadata": {},
   "source": [
    "## XML validieren mit lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd9665",
   "metadata": {},
   "source": [
    "Für diesen Anwendungsfall verwenden wir die ELTeC-Sammlungen als Datensatz. Das Szenario ist die Validierung aller Dateien in einem Ordner mit einem Schema. \n",
    "\n",
    "Die vollständige Textsammlung ELTeC-eng ist hier verfügbar: https://github.com/COST-ELTeC/ELTeC-eng. Im Beispielskript wird auf eine lokale Kopie verwiesen, die 8 Beispieldateien aus ELTeC-eng enthält.\n",
    "\n",
    "Die Schemadatei ist online verfügbar: https://raw.githubusercontent.com/COST-ELTeC/Schemas/master/eltec-1.rng. Auch hier verweisen wir der Einfachheit halber auf eine lokale Kopie. \n",
    "\n",
    "Für das Validieren verwenden wir lxml; siehe die Dokumentation: https://lxml.de/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Importe ===\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from lxml import etree\n",
    "import sys\n",
    "from os.path import join\n",
    "from os.path import basename as bn\n",
    "\n",
    "\n",
    "# === Functions === \n",
    "\n",
    "def parse_xml(xmlfile): \n",
    "    with open(xmlfile, \"r\", encoding=\"utf8\") as infile:\n",
    "        parsed = etree.parse(infile)\n",
    "        return parsed\n",
    "\n",
    "def validate_xml(teiparsed, rngparsed, filename):\n",
    "    # Validierung\n",
    "    rngvalidator = etree.RelaxNG(rngparsed)\n",
    "    validation = rngvalidator.validate(teiparsed)\n",
    "    log = rngvalidator.error_log\n",
    "    # Show results\n",
    "    if validation == True: \n",
    "        print(\"OK, valid: \" + filename)\n",
    "    else:\n",
    "        print(\"\\nSORRY, not valid: \" + filename + \"\\n\" + str(log) + \"\\n\")\n",
    "\n",
    "\n",
    "# === Main ===\n",
    "\n",
    "wdir = join(\"..\", \"data\", \"eltec-eng\")\n",
    "teifilepaths = join(wdir, \"level1\", \"*.xml\")\n",
    "rngfilepath = join(wdir, \"eltec-1.rng\")\n",
    "\n",
    "\n",
    "def main(teifilepaths, rngfilepath): \n",
    "    rngparsed = parse_xml(rngfilepath)\n",
    "    for teifilepath in glob.glob(teifilepaths):\n",
    "        filename = str(bn(teifilepath).split(\".\")[0])\n",
    "        teiparsed = parse_xml(teifilepath)\n",
    "        validate_xml(teiparsed, rngparsed, filename)\n",
    " \n",
    "main(teifilepaths, rngfilepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8eede4",
   "metadata": {},
   "source": [
    "### Aus XML Metadaten oder plain text extrahieren: XPath in lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427817aa",
   "metadata": {},
   "source": [
    "Für diesen Anwendungsfall verwenden wir die ELTeC-Sammlungen (genauer: Ebenfalls die 8 Beispieldateien aus ELTeC-eng) als Datensatz. Die folgenden Szenarien werden betrachtet: \n",
    "\n",
    "1. Die Extraktion von Metadaten aus dem TEI Header, um eine Metadatentabelle anzulegen\n",
    "2. Die Transformation des Textes zu plain text, um weiterführende Textanalysen zu machen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61982ca",
   "metadata": {},
   "source": [
    "### (1) Metadaten aus dem TEI Header extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Importe\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from os.path import join\n",
    "from os.path import basename\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# === Functions ===\n",
    "\n",
    "\n",
    "def open_file(xmlfile): \n",
    "    \"\"\"\n",
    "    Open and parse the XML file. \n",
    "    Returns an XML tree.\n",
    "    \"\"\"\n",
    "    with open(xmlfile, \"r\", encoding=\"utf8\") as infile:\n",
    "        xml = etree.parse(infile)\n",
    "        return xml\n",
    "\n",
    "\n",
    "def get_metadatum(xml, xpath): \n",
    "    \"\"\"\n",
    "    For each XPath, get the metadata item from the XML tree.\n",
    "    Returns a list.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        namespaces = {'tei':'http://www.tei-c.org/ns/1.0',\n",
    "                      'eltec':'http://distantreading.net/eltec/ns'}       \n",
    "        metadatum = xml.xpath(xpath, namespaces=namespaces)[0]\n",
    "    except: \n",
    "        metadatum = \"NA\"\n",
    "    metadatum = re.sub(\"\\.\", \"\", metadatum)\n",
    "    return metadatum\n",
    "\n",
    "\n",
    "def save_metadata(metadata): \n",
    "    \"\"\"\n",
    "    Save all metadata to a CSV file. \n",
    "    \"\"\"\n",
    "    metadatafile = \"ELTeC-eng_metadata.tsv\"\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    print(metadata_df)\n",
    "    with open(metadatafile, \"w\", encoding=\"utf8\") as outfile: \n",
    "        metadata_df.to_csv(outfile, sep=\"\\t\")\n",
    "\n",
    "        \n",
    "# === Main ===\n",
    "\n",
    "teifilepaths = join(\"..\", \"data\", \"eltec-eng\", \"level1\", \"*.xml\")\n",
    "\n",
    "xpaths = {\n",
    "    \"xmlid\" : \"//tei:TEI/@xml:id\", \n",
    "    \"numwords\" : \"//tei:extent/tei:measure[@unit='words']/text()\",\n",
    "    \"sizeCat\" : \"//tei:textDesc/eltec:size/@key\",\n",
    "    \"firsted-yr\" : \"//tei:bibl[@type='firstEdition']/tei:date/text()\",\n",
    "    \"time-slot\" : \"//tei:textDesc/eltec:timeSlot/@key\"\n",
    "    }\n",
    "\n",
    "\n",
    "def main(teifilepaths, xpaths):\n",
    "    allmetadata = []\n",
    "    for teifilepath in glob.glob(teifilepaths): \n",
    "        filename,ext = basename(teifilepath).split(\".\")\n",
    "        try: \n",
    "            keys = [\"filename\"]\n",
    "            metadata = [filename]\n",
    "            xml = open_file(teifilepath)\n",
    "            for key,xpath in xpaths.items(): \n",
    "                metadatum = get_metadatum(xml, xpath)\n",
    "                keys.append(key)\n",
    "                metadata.append(metadatum)\n",
    "            allmetadata.append(dict(zip(keys, metadata)))\n",
    "        except: \n",
    "            print(\"ERROR!!!\", filename)\n",
    "    save_metadata(allmetadata)\n",
    "    \n",
    "main(teifilepaths, xpaths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb29dfd",
   "metadata": {},
   "source": [
    "### Plain text aus einer XML-Datei extrahieren\n",
    "\n",
    "Wir bleiben bei den Beispieldaten und wollen jetzt statt Metadaten aber den Textinhalt extrahieren. \n",
    "\n",
    "Dabei kann man, statt einfach nur alle Elemente zu löschen und den verbleibenden Text zu nehmen, natürlich im XML-Dokument filtern. So kann man beispielsweise festlegen, dass der `teiHeader` komplett ignoriert wird und im `text` beispielsweise nur der Inhalt von `body`, und zwar ohne die Überschriften (`head`) verwendet wird. \n",
    "\n",
    "Im vorliegenden, leicht vereinfachten Beispiel ist das nicht ausführlich parametrisierbar, das ist aber selbstverständlich umsetzbar. Im realen Anwendungsbeispiel kann auch eine automatische Modernisierung der Orthografie vorgenommen werden. Siehe hier: https://github.com/COST-ELTeC/Scripts/tree/master/Python (tei2txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05720002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Importe\n",
    "\n",
    "import os.path\n",
    "import glob\n",
    "from os.path import join\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# === Functions \n",
    "\n",
    "def read_teifile(teifile): \n",
    "    with open(teifile, \"r\", encoding=\"utf8\") as outfile: \n",
    "        tei = etree.parse(teifile)\n",
    "        return tei\n",
    "\n",
    "\n",
    "def remove_tags(tei, nsp): \n",
    "    namespaces = nsp\n",
    "    etree.strip_tags(tei, \"tei:hi\")\n",
    "    etree.strip_tags(tei, \"tei:foreign\")\n",
    "    etree.strip_tags(tei, \"tei:quote\")\n",
    "    return tei\n",
    "\n",
    "\n",
    "def remove_elements(tei, nsp): \n",
    "    namespaces = nsp\n",
    "    etree.strip_elements(tei, \"tei:head\", with_tail=False)\n",
    "    etree.strip_elements(tei, \"tei:note\", with_tail=False)\n",
    "    return tei\n",
    "    \n",
    "\n",
    "def get_text(tei, nsp): \n",
    "    xpath = \"//tei:body//text()\"\n",
    "    text = tei.xpath(xpath, namespaces=nsp)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text): \n",
    "    text = re.sub(\"[ ]{2,20}\", \" \", text)\n",
    "    text = re.sub(\"\\n{2,20}\", \"\\n\", text)\n",
    "    text = re.sub(\"[ \\n]{2,20}\", \" \\n\", text)\n",
    "    text = re.sub(\"\\t{1,20}\", \"\\t\", text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def save_text(text, txtpath, filename): \n",
    "    filename = join(txtpath, filename+\".txt\")\n",
    "    with open(filename, \"w\", encoding=\"utf8\") as outfile: \n",
    "        outfile.write(text)\n",
    "\n",
    "\n",
    "# Parameters \n",
    "\n",
    "teipaths = join(\"..\", \"data\", \"eltec-eng\", \"level1\", \"*.xml\")\n",
    "txtpath = join(\"..\", \"data\", \"eltec-eng\", \"plaintext\", \"\")\n",
    "nsp = {'tei':'http://www.tei-c.org/ns/1.0'}\n",
    "        \n",
    "\n",
    "# === Main \n",
    "\n",
    "def main(teipaths, txtpath, nsp): \n",
    "    if not os.path.exists(txtpath):\n",
    "        os.makedirs(txtpath)\n",
    "    for teifile in glob.glob(teipaths):\n",
    "        filename,ext = os.path.basename(teifile).split(\".\")\n",
    "        tei = read_teifile(teifile)\n",
    "        tei = remove_tags(tei, nsp)\n",
    "        tei = remove_elements(tei, nsp)\n",
    "        text = get_text(tei, nsp)\n",
    "        text = clean_text(text)\n",
    "        print(text[0:500])\n",
    "        save_text(text, txtpath, filename)\n",
    "\n",
    "main(teipaths, txtpath, nsp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bfffc",
   "metadata": {},
   "source": [
    "## XSL-Transformation auf XML anwenden mit lxml\n",
    "\n",
    "Das Beispiel kommt aus einer der vergangenen Sitzungen, in denen Sie mit einem Editor eine XSL-Transformation vorgenommen haben (Baudelaire-Beispiel). Das geht auch mit lxml, wobei lxml aber nur XSLT 1.0 unterstützt, also relativ einfache Stylesheets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf39995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import lxml.etree as et\n",
    "\n",
    "sourcefile = join(\"..\", \"data\", \"xslt\", \"source.xml\")\n",
    "xsltfile = join(\"..\", \"data\", \"xslt\", \"transform.xsl\")\n",
    "outputfile = \"output.html\"\n",
    "\n",
    "def transform_xml(sourcefile, xsltfile, outputfile): \n",
    "\n",
    "    # Read and parse XML sourcefile and XSLT stylsheet\n",
    "    with open(sourcefile, \"r\", encoding=\"utf8\") as infile:\n",
    "        xml = et.parse(infile)\n",
    "    with open(xsltfile, \"r\", encoding=\"utf8\") as infile:\n",
    "        xslt = et.parse(infile)\n",
    "\n",
    "    # Instantiate the transformer and apply to XML\n",
    "    transformer = et.XSLT(xslt)\n",
    "    result = transformer(xml)\n",
    "    print(result)\n",
    "    \n",
    "    # Transform to string and save the result to disk\n",
    "    with open(outputfile, \"w\", encoding=\"utf8\") as outfile:\n",
    "        outfile.write(str(result))\n",
    "    \n",
    "transform_xml(sourcefile, xsltfile, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf31cd0",
   "metadata": {},
   "source": [
    "## Dokumentation generieren: von RNG nach Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440b333",
   "metadata": {},
   "source": [
    "Dieser schon etwas speziellere Anwendungsfall betrifft das automatische Generieren einer menschenlesbaren Dokumentation aus einer Schema-Datei, die in RNG (XML-Syntax) geschrieben ist. \n",
    "\n",
    "Hier ist der reale Code etwas komplexer, als es sich für eine detaillierte Erklärung anbietet. Daher hier nur einige Screenshots sowie einige Links, unter denen Sie sich das anschauen können: \n",
    "\n",
    "* RelaxNG-Datei (XML-Syntax): https://github.com/dh-trier/wlv/blob/master/schemas/wlv-label-schema.rng\n",
    "* Python-Skript zur Transformation: https://github.com/dh-trier/wlv/blob/master/schemas/make-schema-docs.py\n",
    "* \"Einleitung\" in Markdown: https://raw.githubusercontent.com/dh-trier/wlv/master/schemas/wlv-introduction.md\n",
    "* Dokumentation im Ergebnis (nach unten scrollen, um den generierten TEil zu sehen): https://github.com/dh-trier/wlv/blob/master/schemas/wlv-label-docs.md\n",
    "\n",
    "Es ist eigentlich erstaunlich, dass es für diesen Use Case keine Standard-Library gibt. Falls Sie hier etwas entdecken, freue ich mich über Hinweise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27602698",
   "metadata": {},
   "source": [
    "#### Ausschnitt aus dem Schema in RelaxNG\n",
    "\n",
    "![RNG2MD](rng2md-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e0558",
   "metadata": {},
   "source": [
    "#### Ausschnitt aus dem Python-Skript zur Umwandlung\n",
    "\n",
    "![RNG2MD](rng2md-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12fe27",
   "metadata": {},
   "source": [
    "#### Ausschnitt aus der fertigen Dokumentation (Markdown links, Preview rechts)\n",
    "\n",
    "![RNG2MD](rng2md-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49c5ee",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "\n",
    "### Markup und Python\n",
    "\n",
    "1. Arbeiten besser zusammen, als man vielleicht manchmal denkt\n",
    "1. Es gibt eine Reihe von Libraries, die man dafür kennen sollte\n",
    "1. Immer wenn man mehr als eine Handvoll Dateien bearbeiten möchte, oder wenn man eine Routine mehrfach anwenden möchte, lohnt sich der Aufwand der Automatisierung mit Python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python397jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
