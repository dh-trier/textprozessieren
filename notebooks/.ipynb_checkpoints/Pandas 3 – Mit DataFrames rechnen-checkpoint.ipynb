{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f9d0f6",
   "metadata": {},
   "source": [
    "# Pandas 3 – Mit DataFrames rechnen\n",
    "\n",
    "Mit DataFrames kann man nicht nur Metadaten diverser Datentypen bearbeiten, filtern und sortieren, sie eignen sich inbesondere auch sehr gut, um Berechnungen auf größeren Datenmengen einfach und effizient durchzuführen. Darum geht es in diesem Input. \n",
    "\n",
    "Als bekannt vorausgesetzt wird hier: DataFrames aus CSV einlesen, DataFrames aus Listen erstellen, DataFrames sortieren, slicen und filtern, DataFrames abspeichern (siehe Pandas 2). \n",
    "\n",
    "Neu hier ist: Berechnungen auf mehreren ganzen Spalten; Kombination von numpy mit DataFrames; Visualisierung von DataFrames.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb26a40",
   "metadata": {},
   "source": [
    "## Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da95cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2e00a",
   "metadata": {},
   "source": [
    "## Daten: Worthäufigkeiten\n",
    "\n",
    "Als Beispieldatensatz erstellen wir uns eine Worthäufigkeitstabelle aus einem Textkorpus. \n",
    "\n",
    "Das geht am Einfachsten mit der Methode `Count.Vektorizer` aus dem Paket sklearn. Siehe: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3340892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of texts: 12\n",
      "number of words: 5854\n",
      "           acd006  acd012  acd011  acd001  acd010  acd007  acd005  acd004  \\\n",
      "11              0       0       0       3       0       1       1       0   \n",
      "13              1       0       1       4       0       0       0       0   \n",
      "aback           0       0       1       0       0       0       2       1   \n",
      "abandon         2       0       3       3       1       0       4       0   \n",
      "abandoned       4       1       0       3       3       2       1       1   \n",
      "\n",
      "           acd002  acd009  acd008  acd003  \n",
      "11              0       2       0       0  \n",
      "13              0       1       0       0  \n",
      "aback           1       0       0       0  \n",
      "abandon         1       2       0       0  \n",
      "abandoned       4       2       0       0  \n"
     ]
    }
   ],
   "source": [
    "datafolder = join(\"..\", \"data\", \"doyle\", \"\")\n",
    "\n",
    "def create_tdm(datafolder):\n",
    "    # Create idnos and files as lists\n",
    "    files = glob.glob(join(datafolder, \"*.txt\"))\n",
    "    idnos = [os.path.basename(file).split(\".\")[0] for file in files]\n",
    "            \n",
    "    # Define and apply vectorizer\n",
    "    vectorizer = CountVectorizer(input=\"filename\", min_df=4, max_df=10) # min_df, max_df: document frequency\n",
    "    tdm = vectorizer.fit_transform(files)\n",
    "    \n",
    "    # Get vocabulary and transform to TDM / DataFrame\n",
    "    vocab = [item[0] for item in sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1])]\n",
    "    tdm = pd.DataFrame(tdm.toarray().T, columns=idnos, index=vocab)\n",
    "\n",
    "    # Sanity check: inspect results\n",
    "    print(\"number of texts:\", len(files))\n",
    "    print(\"number of words:\", len(vocab))\n",
    "    print(tdm.head())\n",
    "    return tdm\n",
    "\n",
    "tdm = create_tdm(datafolder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2751565",
   "metadata": {},
   "source": [
    "## Metadaten hinzufügen\n",
    "\n",
    "Jetzt haben wir zwar eine Term-Dokument-Matrix, repräsentiert als DataFrame; und wir haben die Spalten und Zeilen auch schon beschriftet. Aber die Kürzel der Dateien sind doch nicht sehr informativ. Es wäre also schön, (a) sie durch die Kurztitel der Romane (oder andere Informationen) zu ersetzen. \n",
    "\n",
    "Wir müssen bedenken, dass die Sortierung der Dokumente in unseren DataFrame und die Sortierung in der Metadatentabelle nicht übereinstimmen müssen. Deswegen sortieren wir beide bevor wir sie verbinden. \n",
    "\n",
    "Weil die Funktion parametrisierbar ist, lassen sich leicht andere Metadaten-Kategorien nutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd026506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           StudyScarlet  HoundBaskervilles  SignFour  WhiteCompany  Refugees  \\\n",
      "11                    0                  0         0             1         0   \n",
      "13                    0                  0         1             0         0   \n",
      "aback                 1                  1         0             2         0   \n",
      "abandon               0                  1         2             4         1   \n",
      "abandoned             1                  4         4             1         3   \n",
      "\n",
      "           FirmGirdlestone  MysteryCloomber  RafflesHaw  Parasite  LostWorld  \\\n",
      "11                       1                2           0         3          0   \n",
      "13                       0                1           0         4          0   \n",
      "aback                    0                0           0         0          0   \n",
      "abandon                  0                2           0         3          0   \n",
      "abandoned                2                2           0         3          1   \n",
      "\n",
      "           ValleyFear  PoisonBelt  \n",
      "11                  0           0  \n",
      "13                  1           0  \n",
      "aback               1           0  \n",
      "abandon             3           0  \n",
      "abandoned           0           0  \n"
     ]
    }
   ],
   "source": [
    "metadatafile = join(datafolder, \"metadata.csv\")\n",
    "target = \"subgenre\" # oder: subgenre, year\n",
    "\n",
    "def add_metadatum(tdm, metadatafile, target): \n",
    "    with open(metadatafile, \"r\", encoding=\"utf8\") as infile: \n",
    "        metadata = pd.read_csv(infile)\n",
    "        metadata.sort_values(by=\"idno\", ascending=True, inplace=True)\n",
    "        #print(metadata.head())\n",
    "    metadatum = list(metadata.loc[:,target])\n",
    "    #print(metadatum)\n",
    "    tdm = tdm.reindex(sorted(tdm.columns), axis=1)\n",
    "    tdm.columns = metadatum\n",
    "    print(tdm.head())\n",
    "    return tdm\n",
    "    \n",
    "tdm = add_metadatum(tdm, metadatafile, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b3911",
   "metadata": {},
   "source": [
    "## Summen bilden: Dokumentlänge, Worthäufigkeit\n",
    "\n",
    "Zunächst einmal können wir auf einer solchen Term-Dokument-Matrix (repräsentiert als DataFrame) die Dokumentlängen und die Häufigkeiten der Wörter im Korpus ermitteln: durch eine zeilenweise oder spaltenweise Summenbildung. \n",
    "\n",
    "Achtung: Wenn wir beim Aufbau der TDM nicht alle Wörter berücksichtigt haben (bspw. wegen Stoplists, minimaler Wortlänge oder minimaler Dokumenthäufigkeit), dann repräsentieren die Zahlen hier natürlich auch nicht die tatsächliche Länge der vollständigen Dokumente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f87262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doclens = np.sum(tdm, axis=0) # 0 = spaltenweise = je Text\n",
    "print(type(doclens),\"\\n\", doclens)\n",
    "\n",
    "wordcounts = np.sum(tdm, axis=1) # 1 = zeilenweise = je Wort\n",
    "print(type(wordcounts), \"\\n\\n\", wordcounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec070a",
   "metadata": {},
   "source": [
    "## Relative Häufigkeiten berechnen\n",
    "\n",
    "Relative Häufigkeiten machen die absoluten Häufigkeiten in Dokumenten unterschiedlicher Länge vergleichbar. Deswegen wird diese Art der \"Normalisierung\" sehr häufig eingesetzt. \n",
    "\n",
    "Wir nehmen die Länge der Dokumente in ihrer Gesamtheit als Grundlage für die Berechnung (siehe oben, `df_min=1`). Wir teilen jede absolute Worthäufigkeit (Zelle) durch die Summe der Worthäufigkeit des relevanten Textes (spaltenweise Summe). \n",
    "\n",
    "Statt Zelle pro Zelle durch den DataFrame zu iterieren, dividieren wir gewissermaßen jede Zeile (als ein Vektor) durch die Liste der Dokumentlänge (auch als ein Vektor). Alle Werte einer Zeile (ein Wert pro Dokument für den relevanten Term) werden also durch unterschiedliche Werte geteilt. Man könnte auch sagen: Wir dividieren die ganze TDM durch einen passenden Vektor in Zeilenlänge. \n",
    "\n",
    "Wir können die Variable `doclens` von oben nachnutzen. Wir fügen die Summe der relativen Häufigkeiten pro Wort als Spalte hinzu, sortieren danach und inspizieren die häufigsten Wörter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Häufigkeiten\n",
    "relfreqs = np.divide(tdm, doclens) * 100 # x100 = Prozent\n",
    "#print(relfreqs.head())\n",
    "\n",
    "# Sanity check: Welche Wörter sind am häufigsten, und wie häufig?\n",
    "relfreqs[\"relfreqsum\"] = np.sum(relfreqs, axis=1)\n",
    "relfreqs.sort_values(by=\"relfreqsum\", inplace=True, ascending=False)\n",
    "relfreqs.drop(\"relfreqsum\", inplace=True, axis=1)\n",
    "print(relfreqs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ece9c9",
   "metadata": {},
   "source": [
    "## Z-Scores berechnen\n",
    "\n",
    "Eine weiter gehende Normalisierung von Worthäufigkeiten sind Z-Scores. \n",
    "\n",
    "Hier werden für jede relative Worthäufigkeit weitere Berechnungen vorgenommen: Man subtrahiert den Mittelwert der Häufigkeit des Wortes von der relativen Häufigkeit (= \"Zentrierung\": der neue Mittelwert wird 0 sein); und man dividiert das Ergebnis durch die Standardabweichung der relativen Häufigkeit des Wortes (= Standardisierung: die neue Standardabweichung wird 1 sein). \n",
    "\n",
    "Wir müssen den DF transponieren (rotieren), um die Subtraktion ausführen zu können. Wir transponieren ihn zurück, um die gleiche TDM zu bekommen.\n",
    "\n",
    "Es gibt viele weitere Möglichkeiten, Worthäufigkeiten zu normaliseren, bspw. TF-IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean relative frequency and standard deviations per word\n",
    "meanfreqs = np.mean(relfreqs, axis=1)\n",
    "stdevs = np.std(relfreqs, axis=1)\n",
    "\n",
    "# Sanity checks: len should be vocabulary, not documents\n",
    "print(len(meanfreqs))\n",
    "print(len(stdevs))\n",
    "\n",
    "# Z-Score transformation: DataFrame by Series\n",
    "zscores = np.divide(np.subtract(relfreqs.T, meanfreqs), stdevs).T\n",
    "#print(zscores.head())\n",
    "\n",
    "# Sanity checks: \n",
    "# - means should be (close to) 0, zeilenweise; Rundungsfehler\n",
    "# - stdevs should be (close) 1, zeilenweise\n",
    "# - min/max should be around -4,+4 not much more\n",
    "zs_means = np.mean(zscores, axis=1)\n",
    "zs_stdevs = np.std(zscores, axis=1)\n",
    "zs_max = np.max(np.max(zscores, axis=1)) # Erst zeilenweise, dann max davon\n",
    "zs_min = np.min(np.min(zscores, axis=1))\n",
    "print(\"means\", list(zs_means)[0:10])\n",
    "print(\"stdevs\", list(zs_stdevs)[0:10])\n",
    "print(\"max\", zs_max)\n",
    "print(\"min\", zs_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6133f",
   "metadata": {},
   "source": [
    "## Visualisierung von Werten\n",
    "\n",
    "Für einfache Visualisierungen von Ergebnissen auf der Basis eines DataFrames kann man direkt pandas nutzen. \n",
    "\n",
    "Grundlage dafür ist die Methode `.plot()`. \n",
    "\n",
    "Siehe: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n",
    "\n",
    "Für weitergehende Visualisierungsmöglichkeiten sind Bibliotheken nützlich, die einen DataFrame als Input nutzen können, darunter: Seaborn, Pygal, Matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dokumentlängen (plot mit Series)\n",
    "\n",
    "title = \"Länge der Dokumente im Korpus\"\n",
    "doclens = np.sum(tdm, axis=0)\n",
    "doclens.plot(kind=\"barh\", title=title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3724dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der relativen Häufigkeiten einzelner Wörter in den Texten\n",
    "\n",
    "term = \"he\"   # \"moor\", \"crime\", \"car\", \"question\"\n",
    "title = \"Relative Häufigkeiten des Terms: \" + term\n",
    "ylabel = \"Relative Häufigkeit in Prozent\"\n",
    "fig1 = relfreqs.loc[term,:].plot(kind=\"bar\", title=title)\n",
    "fig1.set_ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91344aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der Z-Scores eines Wortes in den Texten\n",
    "\n",
    "term = \"she\"   # \"moor\", \"crime\", \"car\", \"question\", \"he\", \"she\"\n",
    "title = \"Z-Scores des Terms: \" + term\n",
    "ylabel = \"Z-Score als Standardabweichung\"\n",
    "\n",
    "fig1 = zscores.loc[term,:].plot(kind=\"bar\", title=title)\n",
    "fig1.set_ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b1512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
