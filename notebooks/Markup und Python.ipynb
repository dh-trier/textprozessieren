{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0327d2",
   "metadata": {},
   "source": [
    "# Markup und/in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2187ed",
   "metadata": {},
   "source": [
    "## Einleitung \n",
    "\n",
    "Dieses Inputreferat ist Teil des Moduls Auszeichnungssprachen im Master of Science \"Digital Humanities\" der Universität Trier. Es ist aber fast ebenso relevant für das Modul \"Programmieren 1: Textprozessieren\". \n",
    "\n",
    "Thema ist die Nutzung und Verarbeitung von Markup-Dateien (wie HTML oder XML) mit Python. Hierfür werden mehrere relevante Libraries vorgestellt und einfache Nutzungsbeispiele gezeigt. \n",
    "\n",
    "Insbesondere geht es um die folgenden Aspekte: \n",
    "\n",
    "1. In HTML-Dateien Informationen suchen mit Regulären Ausdrücken\n",
    "1. HTML-Dateien durchsuchen oder bearbeiten mit BeautifulSoup\n",
    "1. In XML-Dateien mit Python Informationen suchen: XPath in lxml\n",
    "1. Validieren von XML-TEI mit Python: lxml\n",
    "1. XSL-Transformation auf XML anwenden mit lxml\n",
    "1. Dokumentation zu einem Schema generieren: RNG nach Markdown\n",
    "\n",
    "Dieses Inputreferat fokussiert eher auf die Szenarien und Ergebnisse, als auf die Details der konkreten Umsetzung. Vollständige und ausführbare Code-Beispiele werden aber selbstverständlich mit angegeben. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6da6f",
   "metadata": {},
   "source": [
    "## HTML durchsuchen mit Regulären Ausdrücken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f659a",
   "metadata": {},
   "source": [
    "Für diesen Anwendungsfall verwenden wir mehrere aus dem \"Archive of our Own\" heruntergeladene Textdateien in HTML. Das folgenden Szenario wird berücksichtigt:  Extraktion einiger Metadaten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fac4577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AAO-14065050': {'title': 'Lightning - Jetainia - Harry Potter - J K', 'fandom': 'Harry Potter - J. K. Rowling'}, 'AAO-78433': {'title': 'A Series of Sunsets - ChokolatteJedi - Harry Potter - Rowling', 'fandom': 'Harry Potter - Rowling'}, 'AAO-850406': {'title': 'If Harry met Jela - Enleve - Harry Potter and the', 'fandom': 'Harry Potter and the Methods of Rationality'}}\n",
      "                                                          title  \\\n",
      "AAO-14065050          Lightning - Jetainia - Harry Potter - J K   \n",
      "AAO-78433     A Series of Sunsets - ChokolatteJedi - Harry P...   \n",
      "AAO-850406    If Harry met Jela - Enleve - Harry Potter and the   \n",
      "\n",
      "                                                   fandom  \n",
      "AAO-14065050                 Harry Potter - J. K. Rowling  \n",
      "AAO-78433                          Harry Potter - Rowling  \n",
      "AAO-850406    Harry Potter and the Methods of Rationality  \n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from os.path import basename\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "\n",
    "htmlfiles = join(\"..\", \"data\", \"aao\", \"*.html\")\n",
    "\n",
    "def read_html(htmlfile): \n",
    "    \"\"\"\n",
    "    Open and read a single HTML file.\n",
    "    Returns the content as a string.\n",
    "    \"\"\"\n",
    "    with open(htmlfile, \"r\", encoding=\"utf8\") as infile: \n",
    "        html = infile.read()\n",
    "    #print(html[2000:3000])\n",
    "    return html\n",
    "\n",
    "def find_metadata(html): \n",
    "    \"\"\"\n",
    "    Search specific metadata items in the HTML string.\n",
    "    Returns one dictionary with metadata for one document. \n",
    "    \"\"\"\n",
    "    title = re.findall(\"<title>(.*?)</title>\", html)[0]\n",
    "    fandom = re.findall(\"<dt>Fandom:</dt>\\n.*?<dd><a href=.*?>(.*?)</a>\", html)[0]\n",
    "    metadata = {\"title\" : title, \"fandom\": fandom}\n",
    "    return metadata\n",
    "\n",
    "def save_metadata(metadata): \n",
    "    \"\"\"\n",
    "    Transform the dictionary to a DataFrame and save as TSV.\n",
    "    Returns a TSV file saved to disk. \n",
    "    \"\"\"\n",
    "    metadata = pd.DataFrame.from_dict(metadata).T\n",
    "    print(metadata)\n",
    "    with open(\"aao-metadata.tsv\", \"w\", encoding=\"utf8\") as outfile: \n",
    "        metadata.to_csv(outfile, sep=\"\\t\")\n",
    "\n",
    "        \n",
    "def main(htmlfile): \n",
    "    \"\"\"\n",
    "    Collect metadata for a collection of HTML files.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    for htmlfile in glob.glob(htmlfiles):\n",
    "        idno = basename(htmlfile).split(\".\")[0]\n",
    "        html = read_html(htmlfile)\n",
    "        metadata[idno] = find_metadata(html)\n",
    "    print(metadata)\n",
    "    save_metadata(metadata)\n",
    "\n",
    "main(htmlfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbe01b",
   "metadata": {},
   "source": [
    "## Suchen in und Bearbeiten von HTML: mit BeautifulSoup\n",
    "\n",
    "Mit der Library `BeatifulSoup` kann man nicht nur nach Mustern suchen, sondern die Dokumentstruktur des HTML-Dokuments als solche nutzen und wesentlich systematischer Markup-Dateien durchsuchen und bearbeiten. BeautifulSoup funktioniert dabei nicht nur mit HTML, sondern auch mit XML. \n",
    "\n",
    "Dokumentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53875a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "def process_html(html): \n",
    "    hsoup = bs(html, 'html.parser')\n",
    "\n",
    "    # (1) Im HTML-Baum suchen: nach einem Element, Element-Inhalt, etc.\n",
    "    #print(hsoup.h1)\n",
    "    #print(hsoup.title)\n",
    "    #print(hsoup.title.string)\n",
    "    #print(len(hsoup.find_all(\"p\")))\n",
    "\n",
    "    # (2) Allen Text aus dem \"body\" extrahieren (ohne Metadaten)\n",
    "    #print(hsoup.body.get_text())\n",
    "    #print(len(hsoup.body.get_text()))\n",
    "    #text = \"\"\n",
    "    #for item in hsoup.find_all(id=\"chapters\"): \n",
    "    #    text += item.get_text()\n",
    "    #print(len(text))\n",
    "    \n",
    "    # (3) Das HTML-Dokument verändern\n",
    "    element = hsoup.title\n",
    "    #element.clear()      # removes the contents of an element\n",
    "    #element.extract()    # removes the whole element\n",
    "    #hsoup.title.wrap(hsoup.new_tag(\"titleStmt\")) # Ein Element mit zusätzlichem Element umgeben\n",
    "    #print(hsoup)\n",
    "    \n",
    "def main(htmlfiles): \n",
    "    for htmlfile in glob.glob(htmlfiles): \n",
    "        html = read_html(htmlfile)\n",
    "        text = process_html(html)\n",
    "\n",
    "main(htmlfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7ce9f",
   "metadata": {},
   "source": [
    "## XML validieren mit lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd9665",
   "metadata": {},
   "source": [
    "Für diesen Anwendungsfall verwenden wir die ELTeC-Sammlungen als Datensatz. Das Szenario ist die Validierung aller Dateien in einem Ordner mit einem Schema. \n",
    "\n",
    "Die vollständige Textsammlung ELTeC-eng ist hier verfügbar: https://github.com/COST-ELTeC/ELTeC-eng. Im Beispielskript wird auf eine lokale Kopie verwiesen, die 8 Beispieldateien aus ELTeC-eng enthält.\n",
    "\n",
    "Die Schemadatei ist online verfügbar: https://raw.githubusercontent.com/COST-ELTeC/Schemas/master/eltec-1.rng. Auch hier verweisen wir der Einfachheit halber auf eine lokale Kopie. \n",
    "\n",
    "Für das Validieren verwenden wir lxml; siehe die Dokumentation: https://lxml.de/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a5c25e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, valid: ENG18470_Aguilar\n",
      "\n",
      "SORRY, not valid: ENG18400_Trollope\n",
      "/home/christof/Repositories/Github/dh-trier/textprozessieren/data/eltec-eng/level1/ENG18400_Trollope.xml:9:0:ERROR:RELAXNGV:RELAXNG_ERR_ELEMWRONG: Did not expect element author there\n",
      "\n",
      "OK, valid: ENG18450_Disraeli\n",
      "OK, valid: ENG18460_Reynolds\n",
      "OK, valid: ENG18410_Sinclair\n",
      "OK, valid: ENG18471_Bronte\n",
      "OK, valid: ENG18440_Disraeli\n",
      "OK, valid: ENG18411_Tupper\n"
     ]
    }
   ],
   "source": [
    "# === Importe ===\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from lxml import etree\n",
    "import sys\n",
    "from os.path import join\n",
    "from os.path import basename as bn\n",
    "\n",
    "\n",
    "# === Functions === \n",
    "\n",
    "def parse_xml(xmlfile): \n",
    "    with open(xmlfile, \"r\", encoding=\"utf8\") as infile:\n",
    "        parsed = etree.parse(infile)\n",
    "        return parsed\n",
    "\n",
    "def validate_xml(teiparsed, rngparsed, filename):\n",
    "    # Validierung\n",
    "    rngvalidator = etree.RelaxNG(rngparsed)\n",
    "    validation = rngvalidator.validate(teiparsed)\n",
    "    log = rngvalidator.error_log\n",
    "    # Show results\n",
    "    if validation == True: \n",
    "        print(\"OK, valid: \" + filename)\n",
    "    else:\n",
    "        print(\"\\nSORRY, not valid: \" + filename + \"\\n\" + str(log) + \"\\n\")\n",
    "\n",
    "\n",
    "# === Main ===\n",
    "\n",
    "wdir = join(\"..\", \"data\", \"eltec-eng\")\n",
    "teifilepaths = join(wdir, \"level1\", \"*.xml\")\n",
    "rngfilepath = join(wdir, \"eltec-1.rng\")\n",
    "\n",
    "\n",
    "def main(teifilepaths, rngfilepath): \n",
    "    rngparsed = parse_xml(rngfilepath)\n",
    "    for teifilepath in glob.glob(teifilepaths):\n",
    "        filename = str(bn(teifilepath).split(\".\")[0])\n",
    "        teiparsed = parse_xml(teifilepath)\n",
    "        validate_xml(teiparsed, rngparsed, filename)\n",
    " \n",
    "main(teifilepaths, rngfilepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8eede4",
   "metadata": {},
   "source": [
    "### Aus XML Metadaten oder plain text extrahieren: XPath in lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427817aa",
   "metadata": {},
   "source": [
    "Für diesen Anwendungsfall verwenden wir die ELTeC-Sammlungen (genauer: Ebenfalls die 8 Beispieldateien aus ELTeC-eng) als Datensatz. Die folgenden Szenarien werden betrachtet: \n",
    "\n",
    "1. Die Extraktion von Metadaten aus dem TEI Header, um eine Metadatentabelle anzulegen\n",
    "2. Die Transformation des Textes zu plain text, um weiterführende Textanalysen zu machen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61982ca",
   "metadata": {},
   "source": [
    "### (1) Metadaten aus dem TEI Header extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9510fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename     xmlid numwords sizeCat firsted-yr time-slot\n",
      "0   ENG18470_Aguilar  ENG18470   171250    long       1847        T1\n",
      "1  ENG18400_Trollope  ENG18400   207300    long         NA        T1\n",
      "2  ENG18450_Disraeli  ENG18450   158160    long       1845        T1\n",
      "3  ENG18460_Reynolds  ENG18460   839895    long       1846        T1\n",
      "4  ENG18410_Sinclair  ENG18410   188876    long       1841        T1\n",
      "5    ENG18471_Bronte  ENG18471   115398    long       1847        T1\n",
      "6  ENG18440_Disraeli  ENG18440   159046    long       1844        T1\n",
      "7    ENG18411_Tupper  ENG18411    34573   short       1844        T1\n"
     ]
    }
   ],
   "source": [
    "# === Importe\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from os.path import join\n",
    "from os.path import basename\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# === Functions ===\n",
    "\n",
    "\n",
    "def open_file(xmlfile): \n",
    "    \"\"\"\n",
    "    Open and parse the XML file. \n",
    "    Returns an XML tree.\n",
    "    \"\"\"\n",
    "    with open(xmlfile, \"r\", encoding=\"utf8\") as infile:\n",
    "        xml = etree.parse(infile)\n",
    "        return xml\n",
    "\n",
    "\n",
    "def get_metadatum(xml, xpath): \n",
    "    \"\"\"\n",
    "    For each XPath, get the metadata item from the XML tree.\n",
    "    Returns a list.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        namespaces = {'tei':'http://www.tei-c.org/ns/1.0',\n",
    "                      'eltec':'http://distantreading.net/eltec/ns'}       \n",
    "        metadatum = xml.xpath(xpath, namespaces=namespaces)[0]\n",
    "    except: \n",
    "        metadatum = \"NA\"\n",
    "    metadatum = re.sub(\"\\.\", \"\", metadatum)\n",
    "    return metadatum\n",
    "\n",
    "\n",
    "def save_metadata(metadata): \n",
    "    \"\"\"\n",
    "    Save all metadata to a CSV file. \n",
    "    \"\"\"\n",
    "    metadatafile = \"ELTeC-eng_metadata.tsv\"\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    print(metadata_df)\n",
    "    with open(metadatafile, \"w\", encoding=\"utf8\") as outfile: \n",
    "        metadata_df.to_csv(outfile, sep=\"\\t\")\n",
    "\n",
    "        \n",
    "# === Main ===\n",
    "\n",
    "teifilepaths = join(\"..\", \"data\", \"eltec-eng\", \"level1\", \"*.xml\")\n",
    "\n",
    "xpaths = {\n",
    "    \"xmlid\" : \"//tei:TEI/@xml:id\", \n",
    "    \"numwords\" : \"//tei:extent/tei:measure[@unit='words']/text()\",\n",
    "    \"sizeCat\" : \"//tei:textDesc/eltec:size/@key\",\n",
    "    \"firsted-yr\" : \"//tei:bibl[@type='firstEdition']/tei:date/text()\",\n",
    "    \"time-slot\" : \"//tei:textDesc/eltec:timeSlot/@key\"\n",
    "    }\n",
    "\n",
    "\n",
    "def main(teifilepaths, xpaths):\n",
    "    allmetadata = []\n",
    "    for teifilepath in glob.glob(teifilepaths): \n",
    "        filename,ext = basename(teifilepath).split(\".\")\n",
    "        try: \n",
    "            keys = [\"filename\"]\n",
    "            metadata = [filename]\n",
    "            xml = open_file(teifilepath)\n",
    "            for key,xpath in xpaths.items(): \n",
    "                metadatum = get_metadatum(xml, xpath)\n",
    "                keys.append(key)\n",
    "                metadata.append(metadatum)\n",
    "            allmetadata.append(dict(zip(keys, metadata)))\n",
    "        except: \n",
    "            print(\"ERROR!!!\", filename)\n",
    "    save_metadata(allmetadata)\n",
    "    \n",
    "main(teifilepaths, xpaths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb29dfd",
   "metadata": {},
   "source": [
    "### Plain text aus einer XML-Datei extrahieren\n",
    "\n",
    "Wir bleiben bei den Beispieldaten und wollen jetzt statt Metadaten aber den Textinhalt extrahieren. \n",
    "\n",
    "Dabei kann man, statt einfach nur alle Elemente zu löschen und den verbleibenden Text zu nehmen, natürlich im XML-Dokument filtern. So kann man beispielsweise festlegen, dass der `teiHeader` komplett ignoriert wird und im `text` beispielsweise nur der Inhalt von `body`, und zwar ohne die Überschriften (`head`) verwendet wird. \n",
    "\n",
    "Im vorliegenden, leicht vereinfachten Beispiel ist das nicht ausführlich parametrisierbar, das ist aber selbstverständlich umsetzbar. Im realen Anwendungsbeispiel kann auch eine automatische Modernisierung der Orthografie vorgenommen werden. Siehe hier: https://github.com/COST-ELTeC/Scripts/tree/master/Python (tei2txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05720002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "PART I. \n",
      "THE SISTERS. \n",
      "CHAPTER I. \n",
      "A LAUNCH.—A PROMISE.—A NEW RELATION. \n",
      "In a very beautiful part of Wales, between the northern boundaries of Glamorgan and the \n",
      "southeastern of Carmarthenshire, there stood, some twenty or thirty years ago, a small \n",
      "straggling village. Its locality was so completely concealed that the appearance of a \n",
      "gentleman's carriage, or, in fact, any vehicle superior to a light spring-cart, was of such \n",
      "extremely rare occurence as to be dated, in the annals of Llangwilla\n",
      " \n",
      "CHAPTER I. \n",
      "DESCRIPTION OF DOWLING LODGE AND ITS APPURTENANCES — OF ITS MASTER — OF ITS MISTRESS \n",
      "— AND ALL THE MASTERS AND MISSES DOWLING — A LARGE DINNER-PARTY — A HOT \n",
      "DRAWING-ROOM, AND THE WAY TO ESCAPE FROM IT. \n",
      "No traveller can ride or drive within sight of Dowling Lodge, without being tempted \n",
      "to inquire, \"Whose house is that?\" \n",
      "It forms, indeed, a very striking object on the right of the London road, as the hill \n",
      "rises gradually, and overlooks the town of Ashleigh, one of the busiest i\n",
      " \n",
      "Sybil, or the Two Nations \n",
      "BOOK I \n",
      "CHAPTER I. \n",
      "“I’ll take the odds against Caravan.” \n",
      "“In poneys?” \n",
      "“Done.” \n",
      "And Lord Milford, a young noble, entered in his book the bet which he had just made with Mr \n",
      "Latour, a grey headed member of the Jockey Club. \n",
      "It was the eve of the Derby of 1837. In a vast and golden saloon, that in its decorations \n",
      "would have become, and in its splendour would not have disgraced, Versailles in the days of \n",
      "the grand monarch, were assembled many whose hearts beat at th\n",
      " \n",
      "THE MYSTERIES OF LONDON. \n",
      "PROLOGUE. \n",
      "BETWEEN the 10th and 13th centuries Civilisation withdrew from Egypt and Syria, rested for a \n",
      "little space at Constantinople, and then passed away to the western climes of Europe. \n",
      "From that period these climes have been the grand laboratory in which Civilisation has \n",
      "wrought out refinement in every art and every science, and whence it has diffused its benefits \n",
      "over the earth. It has taught commerce to plough the waves of every sea with the adventurous \n",
      "ke\n",
      " \n",
      "  CHAPTER I. \n",
      "The newspapers have recently adopted a strange habit of sometimes unexpectedly seizing an \n",
      "individual's name, long since retired from public notice, and gibbetting it up before the \n",
      "world's eye, when least anticipated, by volunteering a paragraph to announce, that some aged \n",
      "lord, or ex-minister, whom no one has remembered to think of for half a century or more, is \n",
      "residing on his estates, and enjoying, the editor is happy to understand, astonishing health, \n",
      "considering his adva\n",
      " \n",
      "CHAPTER I. \n",
      "1801——I have just returned from a visit to my landlord—the solitary neighbour \n",
      "that I shall be troubled with. This is certainly, a beautiful country! In \n",
      "all England, I do not believe that I could have fixed on a situation so \n",
      "completely removed from the stir of society. A perfect misanthropist's \n",
      "Heaven—and Mr. Heathcliff and I are such a suitable pair to divide the \n",
      "desolation between us. A capital fellow! He little imagined how my heart \n",
      "warmed towards him when I beheld his blac\n",
      " \n",
      "VOL. I. \n",
      "Book I. \n",
      "CHAPTER I. \n",
      "It was a bright May morning some twelve years ago, when a youth of still tender \n",
      "age, for he had certainly not entered his teens by more than two years, was ushered into the \n",
      "waiting-room of a house in the vicinity of St. James's Square, which, though with the general \n",
      "appearance of a private residence, and that too of no very ambitious character, exhibited at \n",
      "this period \n",
      "symptoms of being occupied for some public purpose. \n",
      "The house door was constantly open, an\n",
      " \n",
      "CHAPTER I. \n",
      "PLACE: TIME: CIRCUMSTANCE. \n",
      "Burleigh-Singleton is a pleasant little \n",
      "watering-place on the southern coast of England, \n",
      "entirely suitable for those who have small incomes \n",
      "and good consciences. The latter, to residents \n",
      "especially, are at least as indispensable as the \n",
      "former: seeing that, however just the reputation \n",
      "of their growing little town for superior \n",
      "cheapness in matters of meat and drink, its \n",
      "character in things regarding men and manners is \n",
      "quite as undeniable for pre-e\n"
     ]
    }
   ],
   "source": [
    "# === Importe\n",
    "\n",
    "import os.path\n",
    "import glob\n",
    "from os.path import join\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# === Functions \n",
    "\n",
    "def read_teifile(teifile): \n",
    "    with open(teifile, \"r\", encoding=\"utf8\") as outfile: \n",
    "        tei = etree.parse(teifile)\n",
    "        return tei\n",
    "\n",
    "\n",
    "def remove_tags(tei, nsp): \n",
    "    namespaces = nsp\n",
    "    etree.strip_tags(tei, \"tei:hi\")\n",
    "    etree.strip_tags(tei, \"tei:foreign\")\n",
    "    etree.strip_tags(tei, \"tei:quote\")\n",
    "    return tei\n",
    "\n",
    "\n",
    "def remove_elements(tei, nsp): \n",
    "    namespaces = nsp\n",
    "    etree.strip_elements(tei, \"tei:head\", with_tail=False)\n",
    "    etree.strip_elements(tei, \"tei:note\", with_tail=False)\n",
    "    return tei\n",
    "    \n",
    "\n",
    "def get_text(tei, nsp): \n",
    "    xpath = \"//tei:body//text()\"\n",
    "    text = tei.xpath(xpath, namespaces=nsp)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text): \n",
    "    text = re.sub(\"[ ]{2,20}\", \" \", text)\n",
    "    text = re.sub(\"\\n{2,20}\", \"\\n\", text)\n",
    "    text = re.sub(\"[ \\n]{2,20}\", \" \\n\", text)\n",
    "    text = re.sub(\"\\t{1,20}\", \"\\t\", text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def save_text(text, txtpath, filename): \n",
    "    filename = join(txtpath, filename+\".txt\")\n",
    "    with open(filename, \"w\", encoding=\"utf8\") as outfile: \n",
    "        outfile.write(text)\n",
    "\n",
    "\n",
    "# Parameters \n",
    "\n",
    "teipaths = join(\"..\", \"data\", \"eltec-eng\", \"level1\", \"*.xml\")\n",
    "txtpath = join(\"..\", \"data\", \"eltec-eng\", \"plaintext\", \"\")\n",
    "nsp = {'tei':'http://www.tei-c.org/ns/1.0'}\n",
    "        \n",
    "\n",
    "# === Main \n",
    "\n",
    "def main(teipaths, txtpath, nsp): \n",
    "    if not os.path.exists(txtpath):\n",
    "        os.makedirs(txtpath)\n",
    "    for teifile in glob.glob(teipaths):\n",
    "        filename,ext = os.path.basename(teifile).split(\".\")\n",
    "        tei = read_teifile(teifile)\n",
    "        tei = remove_tags(tei, nsp)\n",
    "        tei = remove_elements(tei, nsp)\n",
    "        text = get_text(tei, nsp)\n",
    "        text = clean_text(text)\n",
    "        print(text[0:500])\n",
    "        save_text(text, txtpath, filename)\n",
    "\n",
    "main(teipaths, txtpath, nsp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bfffc",
   "metadata": {},
   "source": [
    "## XSL-Transformation auf XML anwenden mit lxml\n",
    "\n",
    "Das Beispiel kommt aus einer der vergangenen Sitzungen, in denen Sie mit einem Editor eine XSL-Transformation vorgenommen haben (Baudelaire-Beispiel). Das geht auch mit lxml, wobei lxml aber nur XSLT 1.0 unterstützt, also relativ einfache Stylesheets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bf39995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
      "<meta name=\"Erscheinungsjahr\" content=\"\">\n",
      "</head>\n",
      "<body>\n",
      "<h2>\n",
      "          Charles\n",
      "          Baudelaire\n",
      "        </h2>\n",
      "<h4>À une passante</h4>\n",
      "<p>La rue assourdissante autour de moi hurlait.<br>Longue, mince, en grand deuil, douleur majestueuse,<br>Une femme passa, d’une main fastueuse<br>Soulevant, balançant le feston et l’ourlet ;<br></p>\n",
      "<p>Agile et noble, avec sa jambe de statue.<br>Moi, je buvais, crispé comme un extravagant,<br>Dans son oeil, ciel livide où germe l’ouragan,<br>La douceur qui fascine et le plaisir qui tue.<br></p>\n",
      "<p>Un éclair… puis la nuit ! – Fugitive beauté<br>Dont le regard m’a fait soudainement renaître,<br>Ne te verrai-je plus que dans l’éternité ?<br></p>\n",
      "<p>Ailleurs, bien loin d’ici ! trop tard ! jamais peut-être !<br>Car j’ignore où tu fuis, tu ne sais où je vais,<br>Ô toi que j’eusse aimé, ô toi qui le savais !<br></p>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import lxml.etree as et\n",
    "\n",
    "sourcefile = join(\"..\", \"data\", \"xslt\", \"source.xml\")\n",
    "xsltfile = join(\"..\", \"data\", \"xslt\", \"transform.xsl\")\n",
    "outputfile = \"output.html\"\n",
    "\n",
    "def transform_xml(sourcefile, xsltfile, outputfile): \n",
    "\n",
    "    # Read and parse XML sourcefile and XSLT stylsheet\n",
    "    with open(sourcefile, \"r\", encoding=\"utf8\") as infile:\n",
    "        xml = et.parse(infile)\n",
    "    with open(xsltfile, \"r\", encoding=\"utf8\") as infile:\n",
    "        xslt = et.parse(infile)\n",
    "\n",
    "    # Instantiate the transformer and apply to XML\n",
    "    transformer = et.XSLT(xslt)\n",
    "    result = transformer(xml)\n",
    "    print(result)\n",
    "    \n",
    "    # Transform to string and save the result to disk\n",
    "    with open(outputfile, \"w\", encoding=\"utf8\") as outfile:\n",
    "        outfile.write(str(result))\n",
    "    \n",
    "transform_xml(sourcefile, xsltfile, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf31cd0",
   "metadata": {},
   "source": [
    "## Dokumentation generieren: von RNG nach Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440b333",
   "metadata": {},
   "source": [
    "Dieser schon etwas speziellere Anwendungsfall betrifft das automatische Generieren einer menschenlesbaren Dokumentation aus einer Schema-Datei, die in RNG (XML-Syntax) geschrieben ist. \n",
    "\n",
    "Hier ist der reale Code etwas komplexer, als es sich für eine detaillierte Erklärung anbietet. Daher hier nur einige Screenshots sowie einige Links, unter denen Sie sich das anschauen können: \n",
    "\n",
    "* RelaxNG-Datei (XML-Syntax): https://github.com/dh-trier/wlv/blob/master/schemas/wlv-label-schema.rng\n",
    "* Python-Skript zur Transformation: https://github.com/dh-trier/wlv/blob/master/schemas/make-schema-docs.py\n",
    "* \"Einleitung\" in Markdown: https://raw.githubusercontent.com/dh-trier/wlv/master/schemas/wlv-introduction.md\n",
    "* Dokumentation im Ergebnis (nach unten scrollen, um den generierten TEil zu sehen): https://github.com/dh-trier/wlv/blob/master/schemas/wlv-label-docs.md\n",
    "\n",
    "Es ist eigentlich erstaunlich, dass es für diesen Use Case keine Standard-Library gibt. Falls Sie hier etwas entdecken, freue ich mich über Hinweise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27602698",
   "metadata": {},
   "source": [
    "#### Ausschnitt aus dem Schema in RelaxNG\n",
    "\n",
    "![RNG2MD](rng2md-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e0558",
   "metadata": {},
   "source": [
    "#### Ausschnitt aus dem Python-Skript zur Umwandlung\n",
    "\n",
    "![RNG2MD](rng2md-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12fe27",
   "metadata": {},
   "source": [
    "#### Ausschnitt aus der fertigen Dokumentation (Markdown links, Preview rechts)\n",
    "\n",
    "![RNG2MD](rng2md-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49c5ee",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "\n",
    "### Markup und Python\n",
    "\n",
    "1. Arbeiten besser zusammen, als man vielleicht manchmal denkt\n",
    "1. Es gibt eine Reihe von Libraries, die man dafür kennen sollte\n",
    "1. Immer wenn man mehr als eine Handvoll Dateien bearbeiten möchte, oder wenn man eine Routine mehrfach anwenden möchte, lohnt sich der Aufwand der Automatisierung mit Python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python397jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
